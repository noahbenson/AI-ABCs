{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c278cdf2-5889-4de8-88b2-aa225f118fbb",
   "metadata": {},
   "source": [
    "# Feed-Forward Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1c356c-2fa1-4682-bde6-16303bb76c5c",
   "metadata": {},
   "source": [
    "All of the neural networks we will discuss in this course (including convolutional neural networks and those networks diagramed in the introduction) are feed-forward neural networks, meaning that information always flows forward from one layer to the next, through to the output layer. There are other kinds of neural networks such as recurrent neural networks, which allow information from later (more efferent) layers to be fed back into earlier (more afferent) layers. These kinds of networks typically excel at modeling time-series data, but can take longer to train due to the need to simulate some notion of time for the model during the training steps.\n",
    "\n",
    "This section will introduce the concepts of feed-forward neural networks by example in PyTorch. We will start by building a simple feed-forward PyTorch model then will experiment with various components of neural networks that we can add to it, evaluating its performance on the MNIST dataset as we go."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379f4c37-b2e7-49f0-b902-65a50fb0cc89",
   "metadata": {},
   "source": [
    "## The MNIST Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7794a7-bf9a-40e7-8e38-29f518083cff",
   "metadata": {},
   "source": [
    "For our dataset, we'll use the MNIST image dataset that was introduced in the [previous section](0_introduction). We'll use the same code to load the dataset in here as we did there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b07a85-2b4f-4507-acf7-38b013e82029",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "from pathlib import Path\n",
    "\n",
    "train_dset = MNIST(Path.home(), download=True, train=True, transform=ToTensor())\n",
    "test_dset = MNIST(Path.home(), download=True, train=False, transform=ToTensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520113c1-9359-4a0a-811d-ab5d9af2baa5",
   "metadata": {},
   "source": [
    "## Building a simple linear neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4e8002-c020-4f1c-b0a4-c483535dd21b",
   "metadata": {},
   "source": [
    "To start with, let's build a very simple neural network with no activation function. This will make it functionally no different than a linear transformation, but it's a perfectly good place to start!\n",
    "\n",
    "We'll build the model using PyTorch's `Module` class, and we'll construct internal components that are defined in PyTorch's `torch.nn` subpackage as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fab6d7-6fe8-4853-a620-65a57fc3669a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class NNModel(torch.nn.Module):\n",
    "    \"A simple neural network model for the MNIST dataset.\"\n",
    "    # The image shape for MNIST images is 28x28 pixels; this is the number of\n",
    "    # features in the MNIST inputs.\n",
    "    def __init__(self,\n",
    "                 input_shape=(28, 28),  # The shape of the MNIST images.\n",
    "                 hidden_features=1024,  # The number of hidden layer neurons.\n",
    "                 output_features=10):   # The number of output features.\n",
    "        # Always start Modules with calls to the superclass.\n",
    "        super().__init__()\n",
    "        # The next layer will be a simple linear transformation; i.e., an\n",
    "        # all-to-all set of connections.\n",
    "        input_features = torch.prod(torch.tensor(input_shape))\n",
    "        self.input_to_hidden = torch.nn.Linear(\n",
    "            input_features,\n",
    "            hidden_features)\n",
    "        # Then we transform from hidden features to output features.\n",
    "        self.hidden_to_output = torch.nn.Linear(\n",
    "            hidden_features,\n",
    "            output_features)\n",
    "    def forward(self, inputs):\n",
    "        # Keep in mind that inputs has shape (N, C, H, W) where H and W are\n",
    "        # the image height and width in pixels, N is the batch size, and C is\n",
    "        # the number of channels, which is always 1 for MNIST because they are\n",
    "        # grayscale images.\n",
    "        # First, we want to flatten the image matrix dimensions into a single\n",
    "        # dimension vector, which is how the linear neural network layers\n",
    "        # expect their inputs.\n",
    "        inputs = torch.reshape(inputs, (inputs.shape[0], inputs.shape[1], -1))\n",
    "        # Then we transform from input to hidden layer.\n",
    "        hidden = self.input_to_hidden(inputs)\n",
    "        # Then from hidden to output layer.\n",
    "        output = self.hidden_to_output(hidden)\n",
    "        # At this point, there's no real reason to keep the channels dimension\n",
    "        # as a 1; the output values, one per digit, are the channels.\n",
    "        output = output[:, 0, :]\n",
    "        return output\n",
    "    # We can add a function for predicting the precise digit from the model\n",
    "    # outputs.\n",
    "    def probabilities(self, inputs):\n",
    "        \"\"\"Returns a 10-dimensional vector of probabilities that a particular\n",
    "        input represents each of the 10 digits.\n",
    "\n",
    "        This model's outputs are a 10-element tensor in which each of the 10\n",
    "        dimensions represents one digit; the dimension with the highest value\n",
    "        indicates the model's predicted digit. This function runs the model on\n",
    "        an input and translates the model's output into a confidence\n",
    "        (probability) that the image represents each of the possible digits.\n",
    "        \"\"\"\n",
    "        outputs = model(inputs)\n",
    "        # Keep in mind there will be a batch dimension for inputs and outputs.\n",
    "        # We want to use a sigmoid function to convert these numbers into\n",
    "        # probabilities.\n",
    "        probs = torch.sigmoid(outputs)\n",
    "        # We should also normalize the probabilities.\n",
    "        probs = probs / torch.sum(probs, dim=-1)\n",
    "        return probs\n",
    "    def predict(self, inputs):\n",
    "        \"\"\"Returns the integer digit prediction for the given input tensor.\n",
    "\n",
    "        This model's outputs are a 10-element tensor in which each of the 10\n",
    "        dimensions represents one digit; the dimension with the highest value\n",
    "        indicates the model's predicted digit. This function runs the model on\n",
    "        an input and translates the model's output into a digit.\n",
    "        \"\"\"\n",
    "        outputs = model(inputs)\n",
    "        # Keep in mind there will be a batch dimension for inputs and outputs.\n",
    "        digits = torch.argmax(outputs, dim=-1)\n",
    "        return digits.to(torch.uint8)\n",
    "\n",
    "model = NNModel()\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025fead9-c40e-4bee-8823-dc17de3ebb89",
   "metadata": {},
   "source": [
    "Notice that the number of output channels in the network is 10. This is because we will train the network to produce 10 numbers, one for each digit (0â€“9), and we will understand the model's prediction of the digit represented in an image to be the index of the output with the largest value.\n",
    "\n",
    "In other words, the model will take an image as input and will produce numbers like `output = [12.31, -432.50, 1462.74, -755.75, -20501.06, -9610.48, 837.09, -2768.50,  1409.91,  -7366.86]`. With that output, the largest number is `1462.74`, `output[2]`, and so we take `2` to be the digit that the model predicted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c390e9a-e625-47bf-9cc3-de1a0886285c",
   "metadata": {},
   "source": [
    "## Training the Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ada918c-fbe4-492d-a1d9-2430f84611e1",
   "metadata": {},
   "source": [
    "Training our neural network is going to look substantially similar to training the nonlinear model in the [previous lesson](/3_pytorch/0_introduction). There are a few important differences, however:\n",
    "* Instead of the `SGD` (stochastic gradient descent) optimizer, we'll use the `Adam`, which is a similar gradient-descent-based optimization strategy that is known to work well with neural networks. `Adam`'s interface is almost the same as `SGD`'s.\n",
    "* Instead of the `MSELoss`, we'll use what's called the cross-entropy loss: `CrossEntropyLoss`. This loss function is implemented by PyTorch and is known to work well for evaluating the match of categorical outputs, such as a the prediction of a discrete digit in this case, to a category label. The details of how this loss function works are beyond the scope of this course, but, in brief, the cross entropy loss is low when the model outputs a high value in the channel matching the target and low values in all other channels, and it gets higher the further this is from true. More information can be found [here](https://docs.pytorch.org/docs/2.9/generated/torch.nn.CrossEntropyLoss.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155974f1-776d-400f-bf98-944688c2e2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters:\n",
    "n_epochs = 5       # 1 epoch: you show all your training data to your model once\n",
    "lr = 0.001         # We use a fairly low learning rate.\n",
    "batch_size = 1000  # How many images in one training batch.\n",
    "\n",
    "# Make the model:\n",
    "model = NNModel()\n",
    "\n",
    "# Make the optimizer:\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# Declare our loss function:\n",
    "loss_fn = torch.nn.CrossEntropyLoss(reduction='sum')\n",
    "\n",
    "# Make the dataloaders:\n",
    "train_dloader = torch.utils.data.DataLoader(train_dset, batch_size=batch_size, shuffle=True)\n",
    "test_dloader = torch.utils.data.DataLoader(test_dset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Now we start the optimization loop:\n",
    "for epoch_num in range(n_epochs):\n",
    "    # Put the model in train mode:\n",
    "    model.train()\n",
    "    # In each epoch, we go through each training sample once; the dataloader\n",
    "    # gives these to us in batches:\n",
    "    total_train_loss = 0\n",
    "    for (inputs, targets) in train_dloader:\n",
    "        # We're starting a new step, so we reset the gradients.\n",
    "        optimizer.zero_grad()\n",
    "        # Calculate the model prediction for these inputs.\n",
    "        preds = model(inputs)\n",
    "        # Calculate the loss between the prediction and the actual outputs.\n",
    "        train_loss = loss_fn(preds, targets)\n",
    "        # Have PyTorch backward-propagate the gradients.\n",
    "        train_loss.backward()\n",
    "        # Have the optimizer take a step:\n",
    "        optimizer.step()\n",
    "        # Add up the total training loss:\n",
    "        total_train_loss = total_train_loss + train_loss\n",
    "    mean_train_loss = (total_train_loss / len(train_dset)).detach()\n",
    "    # Now that we've finished training, put the model back in evaluation mode.\n",
    "    model.eval()\n",
    "    # Evaluate the model using the test data.\n",
    "    total_test_loss = 0\n",
    "    for (inputs, targets) in test_dloader:\n",
    "        preds = model(inputs)\n",
    "        test_loss = loss_fn(preds, targets)\n",
    "        total_test_loss = total_test_loss + train_loss\n",
    "    mean_test_loss = (total_test_loss / len(test_dset)).detach()\n",
    "    # Print something about this step:\n",
    "    print(f\"Epoch {epoch_num:2d}:\"\n",
    "          f\"  train loss={mean_train_loss:6.3f};\"\n",
    "          f\"  test loss={mean_test_loss:6.3f}\")\n",
    "# After the optimizer has run, print out what it's found:\n",
    "print(\"Final result:\")\n",
    "print(f\"  train loss = \", float(mean_train_loss))\n",
    "print(f\"   test loss = \", float(mean_test_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39de774-4abe-4571-956b-2d8cc8d92fd9",
   "metadata": {},
   "source": [
    "### Evaluating the Model Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd9b248-268f-41b1-9bbd-773c0473d440",
   "metadata": {},
   "source": [
    "To evaluate our model, we have an overall loss value from the test dataset, but this doesn'e mean anything very specific. Let's try to get a sense of our model's performance by looking at some specific examples from the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5443d8a1-34d9-4ff7-8cbd-c1b408855b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "for k in range(10):\n",
    "    (samp_im, samp_targ) = test_dset[k]\n",
    "    pred = model.predict(samp_im[None, ...])\n",
    "    print(f'Image {k}: {int(pred)} ({samp_targ})')\n",
    "    total += 1\n",
    "    correct += (int(pred) == samp_targ)\n",
    "print(f\"Accuracy for this subset: {correct * 100 / total}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14895355-f263-4dc0-b11b-7b261e416bba",
   "metadata": {},
   "source": [
    "Hopefully it's pretty clear that despite being a pretty simple linear model, our ANN is doing quite well at this classification task!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ceb70e-3f91-404d-ad48-f277474e90f6",
   "metadata": {},
   "source": [
    "## Adding activation functions to our network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d044cac9-84ac-47e5-9ce1-9d5d704fa337",
   "metadata": {},
   "source": [
    "Let's make our neural network a little more interesting. We can add some activation functions to add nonlinearities to the model, potentially allowing it to model more complex relationships. We'll use a very similar network as before, but with a couple more layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533cbfea-5463-4779-83cb-fb372ebae656",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivatedNNModel(torch.nn.Module):\n",
    "    \"A simple neural network model for the MNIST dataset with activation.\"\n",
    "    def __init__(self,\n",
    "                 input_shape=(28, 28),  # The shape of the MNIST images.\n",
    "                 hidden_features=1024,  # The number of hidden layer neurons.\n",
    "                 output_features=10):   # The number of output features.\n",
    "        super().__init__()\n",
    "        input_features = torch.prod(torch.tensor(input_shape))\n",
    "        # Instead of 1 hidden layer, we'll have 2 layers, each with a ReLU\n",
    "        # operator immediately after them.\n",
    "        self.input_to_hidden1 = torch.nn.Linear(\n",
    "            input_features,\n",
    "            hidden_features)\n",
    "        self.relu1 = torch.nn.ReLU()\n",
    "        self.hidden1_to_hidden2 = torch.nn.Linear(\n",
    "            hidden_features,\n",
    "            hidden_features)\n",
    "        self.relu2 = torch.nn.ReLU()\n",
    "        self.hidden2_to_output = torch.nn.Linear(\n",
    "            hidden_features,\n",
    "            output_features)\n",
    "    def forward(self, inputs):\n",
    "        inputs = torch.reshape(inputs, (inputs.shape[0], inputs.shape[1], -1))\n",
    "        hidden1 = self.input_to_hidden1(inputs)\n",
    "        hidden1 = self.relu1(hidden1)\n",
    "        hidden2 = self.hidden1_to_hidden2(hidden1)\n",
    "        hidden2 = self.relu2(hidden2)\n",
    "        output = self.hidden2_to_output(hidden2)\n",
    "        output = output[:, 0, :]\n",
    "        return output\n",
    "    def predict(self, inputs):\n",
    "        \"\"\"Returns the integer digit prediction for the given input tensor.\n",
    "\n",
    "        This model's outputs are a 10-element tensor in which each of the 10\n",
    "        dimensions represents one digit; the dimension with the highest value\n",
    "        indicates the model's predicted digit. This function runs the model on\n",
    "        an input and translates the model's output into a digit.\n",
    "        \"\"\"\n",
    "        outputs = model(inputs)\n",
    "        # Keep in mind there will be a batch dimension for inputs and outputs.\n",
    "        digits = torch.argmax(outputs, dim=-1)\n",
    "        return digits.to(torch.uint8)\n",
    "\n",
    "model = ActivatedNNModel()\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e70fed-7c95-4470-9db8-a30d2124a6fb",
   "metadata": {},
   "source": [
    "Okay, we've made a new model, let's repeat our training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37882bdd-328b-4562-9b1e-610e53b441c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters:\n",
    "n_epochs = 5       # 1 epoch: you show all your training data to your model once\n",
    "lr = 0.001         # We use a fairly low learning rate.\n",
    "batch_size = 1000  # How many images in one training batch.\n",
    "\n",
    "# Make the model:\n",
    "model = ActivatedNNModel()\n",
    "\n",
    "# Make the optimizer:\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# Declare our loss function:\n",
    "loss_fn = torch.nn.CrossEntropyLoss(reduction='sum')\n",
    "\n",
    "# Make the dataloaders:\n",
    "train_dloader = torch.utils.data.DataLoader(train_dset, batch_size=batch_size, shuffle=True)\n",
    "test_dloader = torch.utils.data.DataLoader(test_dset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Now we start the optimization loop:\n",
    "for epoch_num in range(n_epochs):\n",
    "    # Put the model in train mode:\n",
    "    model.train()\n",
    "    # In each epoch, we go through each training sample once; the dataloader\n",
    "    # gives these to us in batches:\n",
    "    total_train_loss = 0\n",
    "    for (inputs, targets) in train_dloader:\n",
    "        # We're starting a new step, so we reset the gradients.\n",
    "        optimizer.zero_grad()\n",
    "        # Calculate the model prediction for these inputs.\n",
    "        preds = model(inputs)\n",
    "        # Calculate the loss between the prediction and the actual outputs.\n",
    "        train_loss = loss_fn(preds, targets)\n",
    "        # Have PyTorch backward-propagate the gradients.\n",
    "        train_loss.backward()\n",
    "        # Have the optimizer take a step:\n",
    "        optimizer.step()\n",
    "        # Add up the total training loss:\n",
    "        total_train_loss = total_train_loss + train_loss\n",
    "    mean_train_loss = (total_train_loss / len(train_dset)).detach()\n",
    "    # Now that we've finished training, put the model back in evaluation mode.\n",
    "    model.eval()\n",
    "    # Evaluate the model using the test data.\n",
    "    total_test_loss = 0\n",
    "    for (inputs, targets) in test_dloader:\n",
    "        preds = model(inputs)\n",
    "        test_loss = loss_fn(preds, targets)\n",
    "        total_test_loss = total_test_loss + train_loss\n",
    "    mean_test_loss = (total_test_loss / len(test_dset)).detach()\n",
    "    # Print something about this step:\n",
    "    print(f\"Epoch {epoch_num:2d}:\"\n",
    "          f\"  train loss={mean_train_loss:6.3f};\"\n",
    "          f\"  test loss={mean_test_loss:6.3f}\")\n",
    "# After the optimizer has run, print out what it's found:\n",
    "print(\"Final result:\")\n",
    "print(f\"  train loss = \", float(mean_train_loss))\n",
    "print(f\"   test loss = \", float(mean_test_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc941458-5d3e-4c30-8b40-47117f983d3a",
   "metadata": {},
   "source": [
    "Clearly this network performs much better! To some extent, this is expected, because the network contains a more complex internal state with more internal parameters. However, the `ReLU` operators contain no parameters themselves; they simply operate over each feature identically.\n",
    "\n",
    "This should demonstrate the importance of simple operators that add nonlinearities to the model structure for the model to exploit. These kinds of operators will continue to be important in the next section."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
