{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "197152bb-512c-4cc5-bec3-64c3979849de",
   "metadata": {},
   "source": [
    "# Introduction to Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af752d17-b339-470b-b9bc-9edd98253b25",
   "metadata": {},
   "source": [
    "Random forests are one of the most flexible and broadly useful AI/ML methods available.\n",
    "They're sometimes considered the \"Swiss army knife\" of supervised AI/ML methods because they are capable of handling all kinds of data and are usually highly accurate even when other methods struggle. \n",
    "In this section, we'll look at how they work and how to apply them to our example dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67384ca-c2b2-4a6e-a292-db4e201ac212",
   "metadata": {},
   "source": [
    "## How do random forests work?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff5a8ef-3479-49ac-b8de-e5a494af7ff7",
   "metadata": {},
   "source": [
    "Random forests are fundamentally made up of several of another kind of machine learning algorithm called a **decision tree**. The idea behind a decision tree is that a set of data can be split into two subsets based on some feature, then the two subsets can each be split on different features, and their subsets can be split, etc., for some depth. After some number of splits in the data, the subsets will be small and a simple classification or regression can be trained for values in that subset. For example, a decision tree modeling the CA Housing Dataset might initially split the data on the number of bedrooms, with all rows with fewer than 3 bedrooms going into one subset and all other rows going into another. These splits can be chosen randomly or using semi-random processes, and they can be generated very quickly.\n",
    "\n",
    "One downside of decision trees is that they are prone to overfitting. Random forests solve this by training many decision trees from distinct randomly chosen subdatasets and averaging them together to form the final model. Although each decision tree is likely to be overfit, they are very unlikely to be overfit in the same features, so their average will usually be less overfit than any one decision tree.\n",
    "\n",
    "Most random forest algorithms use cross validation internally to train and validate the decision trees they create, but you should still use cross validation yourself when training a random forest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d04577-9ff5-4309-8967-34d93b67be04",
   "metadata": {},
   "source": [
    "### Limitations and Advantages of Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690f8e94-9dbf-4970-97cb-d79104b91ec3",
   "metadata": {},
   "source": [
    "**Limitations**\n",
    "* Random forests are very poor at extrapolation.\n",
    "* You can't generally use details about the random forest model to understand how important/influential the model's input parameters are for the output. (If you want to know if a parameter is negatively or positively related to the outputs, then linear regression is a better choice.)\n",
    "\n",
    "**Advantages**\n",
    "* Random forests tend to be highly accurate for many kinds of data.\n",
    "* Random forests tend to be very robust to outliers.\n",
    "* Random forests usually handle missing data well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42df4d90-9640-4d96-90fe-39c58d544c81",
   "metadata": {},
   "source": [
    "## Example: the California Housing Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3376d7a9-4a01-4940-ae46-4f741bc663e1",
   "metadata": {},
   "source": [
    "We'll use a random forest to try to predict the median housing prices in the CA Housing Dataset. We can start by loading the dataset as usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f4b46a-d70d-4c56-95c4-ab8e233ba22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn as skl\n",
    "\n",
    "# We use scikit-learn to download and return the CA housing dataset:\n",
    "ca_housing_dataset = skl.datasets.fetch_california_housing()\n",
    "\n",
    "# Extract the actual data rows and the feature names:\n",
    "ca_housing_featdata = ca_housing_dataset['data']\n",
    "ca_housing_featnames = ca_housing_dataset['feature_names']\n",
    "\n",
    "# We also extract the \"target\" data, since we are using supervised learning:\n",
    "ca_housing_targdata = ca_housing_dataset['target']\n",
    "ca_housing_targnames = ca_housing_dataset['target_names']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8123ba26-b3cb-4753-99ea-ffb5301782ea",
   "metadata": {},
   "source": [
    "As in the previous section, we'll split the dataset into train and test subdatasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990e1b8f-c35e-4f40-9afb-9933f0480d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Randomly select 75% of the rows to be in the training dataset.\n",
    "all_rows = np.arange(ca_housing_featdata.shape[0])\n",
    "n_train = int(round(len(all_rows) * 0.75))\n",
    "n_test = len(all_rows) - n_train\n",
    "train_rows = np.random.choice(all_rows, n_train, replace=False)\n",
    "test_rows = np.setdiff1d(all_rows, train_rows)\n",
    "\n",
    "# Extract these rows into separate matrices:\n",
    "train_featdata = ca_housing_featdata[train_rows]\n",
    "train_targdata = ca_housing_targdata[train_rows]\n",
    "test_featdata = ca_housing_featdata[test_rows]\n",
    "test_targdata = ca_housing_targdata[test_rows]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d71da07-2f25-4189-90f7-b6d651fefd70",
   "metadata": {},
   "source": [
    "Next, we can create the random forest management object. In this case, we'll want to give the management object a few *hyperparameters*, specifically the `max_depth`, which tells it how many times to split the data in the decision trees, and `random_state`, which can be used to ensure that randomized choices made by the algorithm are repeatable. We'll use `random_state=0` here, but you can use different random states to see how the algorithm varies across random runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944ba812-5042-4f90-8e19-7dd5b0336776",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "randforest = RandomForestRegressor(\n",
    "    max_depth=6,\n",
    "    random_state=0)\n",
    "\n",
    "# Next, we train the random forest with our data.\n",
    "randforest.fit(train_featdata, train_targdata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d91ce8-e16f-47b2-b79e-ae5c2d726e3e",
   "metadata": {},
   "source": [
    "There are a variety of other hyperparameters that can be given to the `RandomForestRegressor`, such as the number of estimators to make and average together (`n_estimators`), that are outside the scope of this lesson. See the [Scikit-learn documentation on random forests](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html) for more information on these options.\n",
    "\n",
    "For now, let's see how well the model performs on our test dataset. Like with the `LinearRegression` type, we can use the `score` method to obtain the coefficient of determination for the model and the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427226e9-99b7-40bb-91bc-25bedddbd42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "randforest.score(test_featdata, test_targdata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2362f98-2db9-45ed-895e-9c262d01dbe1",
   "metadata": {},
   "source": [
    "The random forest appears to explain about 70â€“71% of the variance in the test dataset; that's somewhat better than the linear regression model we saw earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfe9149-d017-4ac2-8caf-cdd293c2695a",
   "metadata": {},
   "source": [
    "### What kind of data does the `RandomForestRegressor` provide?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1755e3e3-6012-4498-8c29-f8f4905dfe35",
   "metadata": {},
   "source": [
    "As we've seen with other ML tools in Scikit-learn, the `RandomForestRegressor` provides us with some data about the trained model. In the `LinearRegression` type, these data were the coefficients associated with each feature in the regression. For random forests, these data are the individual decision tree estimators. We can see the list of estimators by examining the `estimators_` member variable of the `randforest` object we created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2cee86-5f9a-4916-9b11-e048bb660fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "randforest.estimators_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d9f29b-16b4-4d52-82e3-9dc7c7bbf44c",
   "metadata": {},
   "source": [
    "As we can see, the `estimators_` variable is a list of `DecisionTreeRegressor` objects. This makes sense, since random forests are just collections of decision trees. Let's take a closer look at one of the trees in our forest. The Scikit-learn library includes a function `plot_tree` in the `sklearn.tree` subpackage that can be used to visualize a decision tree as part of a `matplotlib` figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b61fff-11a6-4c52-acd0-2af970b9c545",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# We'll look at the first tree:\n",
    "tree = randforest.estimators_[0]\n",
    "\n",
    "# Make a figure; we have to make the figure quite large in order for all of\n",
    "# the text and all the nodes in the tree to be visible!\n",
    "(fig,ax) = plt.subplots(1, 1, figsize=(24,12), dpi=72*8)\n",
    "\n",
    "# Plot the tree:\n",
    "skl.tree.plot_tree(tree, ax=ax)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49912e67-7eff-45a0-a7be-e5c93798563f",
   "metadata": {},
   "source": [
    "The tree has very small text in its cells, so you may need to open the image in a new browser tab and zoom in in order to read it. Essentially, each node in the tree details the condition for splitting the data. In the root node of the tree, for example, the data are split according to the rule `x[0] <= 4.537`; the `x[0]` here indicates the first feature used in the training (the median income in a region of CA for our dataset).\n",
    "\n",
    "One of the nice features of decision trees and random forests is that the trees themselves can be examined and understood&mdash;just by looking through the nodes of this tree, we can get a general sense of how the algorithm has decided to calculate a prediction. The Scikit-learn library additionally includes a number of utilities and tutorials related to decision trees and how to evaluate and examine them. In particular, more information on the structure of the decision trees can be found [here](https://scikit-learn.org/stable/auto_examples/tree/plot_unveil_tree_structure.html) and more general information on decision trees can be found [here](https://scikit-learn.org/stable/auto_examples/tree/plot_tree_regression.html)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
