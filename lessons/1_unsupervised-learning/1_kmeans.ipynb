{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ba96efa-707e-4046-9081-4afa9c745959",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# k-Means Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0eed309-e12d-4dc6-b5ed-d3255a752968",
   "metadata": {},
   "source": [
    "The k-means clustering algorithm is one of the most intuitive unsupervised learning algorithm. It's goal is to deduce natural clusters (also called \"classes\") in a dataset by figuring out which data points are most similar to each other. It's called \"k-means\" because it results in $k$ classes using a method that attempts to find the mean of each class of data points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc30848-3e28-4ced-9392-495f95c94ff8",
   "metadata": {},
   "source": [
    "## How It Works"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29027a13-a3c6-4655-9a70-99e1110828d1",
   "metadata": {},
   "source": [
    "Many machine learning methods work via a similar pattern: (1) they make a guess about the answer, (2) they make a small change to the guess that improves its goodness slightly, then (3) they repeat step 2 until the guess is \"good enough\" in some way. The k-means algorithm is a great example of this approach. It works as follows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08525d38-3fb5-43b3-a7e2-a15488b880f1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### K-Means Algorithm\n",
    "\n",
    "#### Inputs\n",
    "1. **A set of points**. There may be any number of points, and they may have any number of dimensions (columns). All the dimensions should be numerical.\n",
    "2. **$k$**. The number of clusters to find.\n",
    "\n",
    "#### Algorithm\n",
    "1. **Guess**. The algorithm randomly chooses $k$ points. These \"cluster points\" will have the same dimensionality as the input points.\n",
    "2. **Iterate**. These steps are performed repeatedly until some threshold is reached.\n",
    "   1. **Assign clusters**. For each point in the input, the algorithm figures out which of the $k$ cluster points is closest to it and assigns it that cluster value.\n",
    "   2. **Update clusters**. For each cluster, update the position of the cluster point to be the mean of all input points that belong to the cluster.\n",
    "\n",
    "#### Outputs\n",
    "1. The $k$ cluster points representing the $k$ clusters found by the method; each data point is the mean position of all the data points in one of the $k$ clusters.\n",
    "2. An assignment of all the input points into one of the $k$ clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75f0042-b73b-4958-b8de-83e2d0f57071",
   "metadata": {},
   "source": [
    "### Demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7869674-7597-4fb4-b8fd-ce22d6d66569",
   "metadata": {},
   "source": [
    "The following animation demonstrates how the k-means algorithm works generally.\n",
    "\n",
    "In this animation, the black dots are the data points that we are hoping to cluster based on their `(x,y)` position. Each star represents one of the three clusters.\n",
    "\n",
    "Initially, the three stars have a random position that doesn't do a very good job of representing where the underlying clusters are. However, after a few iterations of the k-means algorithm, in which we repeatedly assign each data point to its nearest cluster then move each cluster's position to the mean position of its assigned data points, we reach a good clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb06377-8c27-4cd8-a6b3-b60c5233897d",
   "metadata": {
    "editable": true,
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# This code-block generates and displays an animation of the K-Means algorithm\n",
    "# for a random set of points.\n",
    "# This specific code uses the `matplotlib.animation.FuncAnimation` class to\n",
    "# generate the animation. A very simple implementation of the k-means\n",
    "# clustering algorithm can be found in the `run_kmeans` function below, but\n",
    "# understanding this code isn't important for the K-Means lesson.\n",
    "\n",
    "def _make_kmeans_animation(seed=5, n=100):\n",
    "    import numpy as np\n",
    "    from scipy.spatial import KDTree\n",
    "    import ipywidgets as ipw\n",
    "    import matplotlib.pyplot as plt\n",
    "    from matplotlib.animation import FuncAnimation\n",
    "    from IPython.display import HTML\n",
    "    \n",
    "    # The random seed:\n",
    "    np.random.seed(seed)\n",
    "    # Number of data points:\n",
    "    #n = 100\n",
    "    # Number of clusters:\n",
    "    k = 3\n",
    "    # The number of steps:\n",
    "    s = 6\n",
    "    # x/y min and max:\n",
    "    (xmin,xmax) = (-10, 10)\n",
    "    (ymin,ymax) = (-10, 10)\n",
    "    # The colors for each cluster:\n",
    "    clust_colors = np.array([(1, 0.2, 0.2), (0, 0.8, 0.8), (0.7, 0.7, 0.7)])\n",
    "    \n",
    "    def random_datapoints_clustered(center=None, scale=None,\n",
    "                                    n=n, out=None,\n",
    "                                    xmin=xmin, xmax=xmax, ymin=ymin, ymax=ymax):\n",
    "        # Draw a random center:\n",
    "        if center is None:\n",
    "            center = np.random.rand(2) * [xmax - xmin, ymax - ymin] + [xmin, ymin]\n",
    "        else:\n",
    "            center = np.array(center)\n",
    "        # Pick a random scale:\n",
    "        if scale is None:\n",
    "            scale = np.random.exponential(2.0)\n",
    "        # Draw some angles and radii.\n",
    "        th = np.random.rand(n) * (2*np.pi)\n",
    "        r = np.random.randn(n) * scale\n",
    "        # Convert these to (x,y):\n",
    "        xy = center[:,None] + [r*np.cos(th), r*np.sin(th)]\n",
    "        # Ready the output:\n",
    "        if out is None:\n",
    "            out = np.empty_like(xy)\n",
    "        # Make sure none of the xy are out of range:\n",
    "        return np.stack(\n",
    "            [np.mod(xy[0] - xmin, xmax - xmin) + xmin,\n",
    "             np.mod(xy[1] - ymin, ymax - ymin) + ymin],\n",
    "            axis=0,\n",
    "            out=out)\n",
    "    def random_datapoints(centers=None, scales=None,\n",
    "                          n=n, k=k, out=None,\n",
    "                          xmin=xmin, xmax=xmax, ymin=ymin, ymax=ymax):\n",
    "        if out is None:\n",
    "            out = np.empty((2, n), dtype=float)\n",
    "        # We make k clusters...\n",
    "        step = n // k\n",
    "        for clustno in range(k - 1):\n",
    "            random_datapoints_clustered(\n",
    "                center=(None if centers is None else centers[clustno]),\n",
    "                scale=(None if scales is None else scales[clustno]),\n",
    "                n=step,\n",
    "                out=out[:,(clustno*step):((clustno+1)*step)],\n",
    "                xmin=xmin, xmax=xmax, ymin=ymin, ymax=ymax)\n",
    "        random_datapoints_clustered(\n",
    "            center=(None if centers is None else centers[k-1]),\n",
    "            scale=(None if scales is None else scales[k-1]),\n",
    "            n=(out.shape[1] - (k-1)*step), \n",
    "            out=out[:,((k-1)*step):],\n",
    "            xmin=xmin, xmax=xmax, ymin=ymin, ymax=ymax)\n",
    "        return out\n",
    "    def random_clusterpoints(k=k, xmin=xmin, xmax=xmax, ymin=ymin, ymax=ymax):\n",
    "        return np.random.rand(2,k) * [[xmax-xmin],[ymax-ymin]] + [[xmin],[ymin]]\n",
    "    \n",
    "    # Also make the data and cluster points we will use:\n",
    "    coords = random_datapoints([(8,4), (2,-6), (-6,-4)], [2, 2, 2], n=n)\n",
    "    clust0 = random_clusterpoints()\n",
    "    clusts = np.empty_like(clust0, shape=((s + 1,) + clust0.shape))\n",
    "    labels = np.zeros((s + 1, n), dtype=np.uint8)\n",
    "    # Actually perform the k-means clustering steps:\n",
    "    def run_kmeans(clust0, n=n, k=k, s=s,\n",
    "                   coords=coords, clusts=clusts, labels=labels):\n",
    "        clusts[0] = clust0\n",
    "        for stepno in range(s):\n",
    "            near = KDTree(clusts[stepno].T)\n",
    "            (d,ii) = near.query(coords.T)\n",
    "            labels[stepno] = ii\n",
    "            clusts[stepno + 1] = np.stack(\n",
    "                [np.mean(coords[:, labels[stepno] == ll], axis=1)\n",
    "                 for ll in range(k)],\n",
    "                axis=1)\n",
    "        near = KDTree(clusts[s].T)\n",
    "        (d,ii) = near.query(coords.T)\n",
    "        labels[s] = ii\n",
    "    run_kmeans(clust0)\n",
    "    \n",
    "    # Make the figure we'll use:\n",
    "    (fig,ax) = plt.subplots(1,1, figsize=(3, 3), dpi=180)\n",
    "    fig.subplots_adjust(0,0,1,1,0,0)\n",
    "    ax.set_xlim([xmin-0.5, xmax+0.5])\n",
    "    ax.set_ylim([ymin-0.5, ymax+0.5])\n",
    "    ax.plot([xmin, xmax], [0, 0], 'k:', lw=0.25, zorder=-1)\n",
    "    ax.plot([0, 0], [ymin, ymax], 'k:', lw=0.25, zorder=-1)\n",
    "    ax.axis('off')\n",
    "    # Draw a few things we'll use in all the figure frames:\n",
    "    coordplot = ax.scatter(\n",
    "        coords[0], coords[1], \n",
    "        fc='k', s=8, lw=0.5, edgecolor=None)\n",
    "    clustplot = ax.scatter(\n",
    "        clusts[0,0], clusts[0,1],\n",
    "        fc=clust_colors, s=50, lw=0.5, ec='k', marker='*')\n",
    "    arrowplots = [\n",
    "        ax.arrow(\n",
    "            clusts[0,0,ii], clusts[0,1,ii], 1, 1,\n",
    "            lw=0.5, color=clr, head_width=0.1)\n",
    "        for (ii,clr) in enumerate(clust_colors)]\n",
    "    textplot = ax.text(\n",
    "        -9.5, 7.5, \"Round\",\n",
    "        fontsize=10, horizontalalignment='left', verticalalignment='top')\n",
    "    \n",
    "    # We can now define how to draw a frame; there are actually three kinds of\n",
    "    # frames: (1) basic plots, (2) cluster-step plots, and (3) update-step plots.\n",
    "    def basic_frameplot(stepno,\n",
    "                        coords=coords, clusts=clusts,\n",
    "                        fig=fig, ax=ax, clustplot=clustplot,\n",
    "                        xmin=xmin, xmax=xmax, ymin=ymin, ymax=ymax):\n",
    "        x = clusts[stepno, 0]\n",
    "        y = clusts[stepno, 1]\n",
    "        clustplot.set_offsets(np.c_[x,y])\n",
    "        for arrow in arrowplots:\n",
    "            arrow.set_visible(False)\n",
    "        coordplot.set_edgecolor(None)\n",
    "        textplot.set_text(f\"Round {stepno}: Start\")\n",
    "    def cluster_frameplot(stepno,\n",
    "                          coords=coords, clusts=clusts,\n",
    "                          fig=fig, ax=ax, clustplot=clustplot,\n",
    "                          xmin=xmin, xmax=xmax, ymin=ymin, ymax=ymax):\n",
    "        x = clusts[stepno, 0]\n",
    "        y = clusts[stepno, 1]\n",
    "        clustplot.set_offsets(np.c_[x,y])\n",
    "        lbl = labels[stepno]\n",
    "        coordplot.set_edgecolor(clust_colors[lbl])\n",
    "        for arrow in arrowplots:\n",
    "            arrow.set_visible(False)\n",
    "        textplot.set_text(f'Round {stepno}: Assign Clusters')\n",
    "    def update_frameplot(stepno,\n",
    "                         coords=coords, clusts=clusts,\n",
    "                         fig=fig, ax=ax, clustplot=clustplot,\n",
    "                         xmin=xmin, xmax=xmax, ymin=ymin, ymax=ymax):\n",
    "        (x0, y0) = clusts[stepno]\n",
    "        (x, y) = clusts[stepno + 1]\n",
    "        (dx, dy) = (x - x0, y - y0)\n",
    "        for (ii,arrow) in enumerate(arrowplots):\n",
    "            arrow.set_data(x=x0[ii], y=y0[ii], dx=dx[ii], dy=dy[ii])\n",
    "            arrow.set_visible(True)\n",
    "        coordplot.set_edgecolor(clust_colors[labels[stepno]])\n",
    "        textplot.set_text(f\"Round {stepno}: Update Cluster Means\")\n",
    "    # Draw the step 0 frame:\n",
    "    def draw_frame0(coords=coords, clusts=clusts,\n",
    "                    fig=fig, ax=ax, clustplot=clustplot,\n",
    "                    xmin=xmin, xmax=xmax, ymin=ymin, ymax=ymax):\n",
    "        basic_frameplot(\n",
    "            0,\n",
    "            coords=coords, clusts=clusts,\n",
    "            fig=fig, ax=ax, clustplot=clustplot,\n",
    "            xmin=xmin, xmax=xmax, ymin=ymin, ymax=ymax)\n",
    "        textplot.set_text(\"Initial State\")\n",
    "    def frameplot(stepno,\n",
    "                  coords=coords, clusts=clusts,\n",
    "                  fig=fig, ax=ax,\n",
    "                  xmin=xmin, xmax=xmax, ymin=ymin, ymax=ymax):\n",
    "        opts = dict(\n",
    "            coords=coords, clusts=clusts,\n",
    "            fig=fig, ax=ax,\n",
    "            xmin=xmin, xmax=xmax, ymin=ymin, ymax=ymax)\n",
    "        if stepno == 0:\n",
    "            draw_frame0(**opts)\n",
    "        elif stepno == 9:\n",
    "            cluster_frameplot(4, **opts)\n",
    "            textplot.set_text('Final Clusters')\n",
    "        elif stepno % 3 == 0:\n",
    "            basic_frameplot(stepno // 3, **opts)\n",
    "        elif stepno % 3 == 1:\n",
    "            cluster_frameplot(stepno // 3, **opts)\n",
    "        else:\n",
    "            update_frameplot(stepno // 3, **opts)\n",
    "        return [textplot, coordplot, clustplot] + arrowplots\n",
    "    \n",
    "    anim = FuncAnimation(\n",
    "        fig,\n",
    "        frameplot,\n",
    "        frames = 10,\n",
    "        interval = 1500,\n",
    "        blit = True)\n",
    "    plt.close(fig)\n",
    "    display(HTML(anim.to_jshtml()))\n",
    "\n",
    "# Call the above function, which displays the javascript animation.\n",
    "_make_kmeans_animation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73918d32-592d-4b3b-b5ce-4ace57db1aec",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Example: Finding Clusters in the California Housing Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c856f00-19b0-4765-abbc-ab4c24ec94cd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Let's work through an example of k-means clustering with a real dataset. We'll use the California Housing Dataset that we examined earlier in the [Introduction to Unsupervised Learning](0_introduction), and we'll attempt to find geographical clusters of census parcels using k-means clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886185ce-ad5f-451f-bda1-7b6dc1fc0bc4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2b7623-4550-4f3c-8e59-be0828f2072c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we need to import scikit-learn:\n",
    "import sklearn as skl\n",
    "\n",
    "# Next, we use scikit-learn to download and return the CA housing dataset:\n",
    "ca_housing_dataset = skl.datasets.fetch_california_housing()\n",
    "\n",
    "# Extract the actual data rows and the feature names:\n",
    "ca_housing_data = ca_housing_dataset['data']\n",
    "ca_housing_featnames = ca_housing_dataset['feature_names']\n",
    "\n",
    "# We also extract the target data:\n",
    "ca_housing_targdata = ca_housing_dataset['target']\n",
    "ca_housing_targnames = ca_housing_dataset['target_names']\n",
    "\n",
    "# To organize the dataset into a dataframe, we use Pandas:\n",
    "import pandas as pd\n",
    "\n",
    "feat_df = pd.DataFrame(\n",
    "    {k: v for (k,v) in zip(ca_housing_featnames, ca_housing_data.T)})\n",
    "targ_df = pd.DataFrame({ca_housing_targnames[0]: ca_housing_targdata})\n",
    "\n",
    "# Display the feature dataframe:\n",
    "feat_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a9baeb-8f6d-4f55-ac14-977f906c210d",
   "metadata": {},
   "source": [
    "### Visualize the Longitude and Latitude Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d948dca-ddea-4097-9a71-101eb0df7b1f",
   "metadata": {},
   "source": [
    "We can start by visualizing the raw data that we're going to use for clustering. We're using the longitude and latitude of each census parcel in the dataset, so a visualization of these coordinates should be resemble the state of California."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6a47ae-2136-4635-8fc6-72e9c3dc3735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the longitude and latitude from the dataset features.\n",
    "x = feat_df['Longitude']\n",
    "y = feat_df['Latitude']\n",
    "\n",
    "# We'll need matplotlib to make the plots:\n",
    "import matplotlib.pyplot as plt\n",
    "# Make a quick scatter-plot where each dot is a parcel center.\n",
    "plt.scatter(x, y, c='k', s=0.5)\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "# Make sure the plot axis represents x and y space equally.\n",
    "plt.axis('equal')\n",
    "# Show the plot.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da19948-fa56-4a9c-bbe4-69ace466fc4c",
   "metadata": {},
   "source": [
    "Clearly there are clusters of parcels near the most populated parts of the state: Los Angeles, San Diego, San Francisco and the Bay Area, and Sacramento in particular. While there is no one correct way to cluster the parcels in this map, we might assess that a method of clustering performs well if it tends to identify these areas as distinct clusters.\n",
    "\n",
    "Let's see how to use k-means for this task and how well it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308d1286-695e-4b06-870a-2d390c4cfae6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Make an (Nx2) coordinate matrix of the latitude and longitude:\n",
    "coords = np.c_[x,y]\n",
    "# Use scikit-learn's KMeans algorithm; we'll search for 8 clusters:\n",
    "n_clusters = 8\n",
    "kmeans = skl.cluster.KMeans(n_clusters=n_clusters)\n",
    "kmeans.fit(coords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9456ed3-eae8-4864-a8cb-9ffde5706d25",
   "metadata": {},
   "source": [
    "Once the above cell has been run, the `kmean` object has been fitted, meaning that it has found cluster centers and cluster labels for the parcels in the census. We can access these discovered values as fields of the `kmeans` object itself.\n",
    "\n",
    "In scikit-learn, this paradigm is very common: one creates an object (i.e., a `sklearn.cluster.KMeans` object) that takes some parameters (the number of clusters), then you provide the object with data via a `fit` function. After calling the fitting routine, the object has been trained and its methods and fields can be queried.\n",
    "\n",
    "The fields that scikit-learn provides for the user typically end with an underscore (`_`) character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb036850-06e5-4bdc-bf56-297abc0268f3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This gives us one label per parcel in the census.\n",
    "label = kmeans.labels_\n",
    "# And one cluster center per cluster.\n",
    "centers = kmeans.cluster_centers_\n",
    "\n",
    "# Make another scatter plot of the parcels, but color them by their cluster\n",
    "# label this time:\n",
    "plt.scatter(x, y, c=label, s=0.5, cmap='hsv')\n",
    "# Also plot the cluster centers as stars.\n",
    "plt.scatter(\n",
    "    centers[:,0], centers[:,1],\n",
    "    edgecolor='k',\n",
    "    c=np.linspace(0,1,n_clusters),\n",
    "    cmap='hsv', \n",
    "    s=50,\n",
    "    marker='*')\n",
    "# Fix up the plot to use isomorphic pixel sizes.\n",
    "plt.axis('equal')\n",
    "# And plot the figure.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c4b2c1-a12a-4ab7-bd3e-24c1a9d81e78",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Clearly, the algorithm found some clusters that make sense, but it's simultaneously hard to say whether these clusters are \"correct\" in any meaningful sense. In part, this is because geographic clustering is a complicated topic&mdash;politics, culture, geography, and population all play major roles in our societal notion of a geographical regions. However, k-means is also limited in that it cares only about a simple metric of closeness, and it's simply not always the case that spatial closeness is a good indicator of whether two locations are part of a geographical cluster.\n",
    "\n",
    "Note additionally that k-means is a stochastic algorithm, meaning that if you run it several times, you will likely get slightly different results each time. If you run the two cells above several times, you can get a sense for the kinds of clusterings k-means can produce from similar data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253212f9-0865-40f9-937c-36245092e48f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "**After running the cell above several times, what are some of the strengths and weaknesses of this method that you observe?**\n",
    "\n",
    "```{dropdown} Some possible strengths...\n",
    "\n",
    "* **K-means runs quickly**. In fact, you can run it many times even for fairly large datasets.\n",
    "* **K-means classifies all points**. There is never an outlier point that the k-means algorithm misses or fails to classify.\n",
    "* **K-means is easy to understand**. The k-means method itself is straightforward, and others will have little trouble understanding results based on k-means.\n",
    "```\n",
    "\n",
    "```{dropdown} Some possible weaknesses...\n",
    "\n",
    "* **K-means is not especially reliable**. The answer you get differs quite a bit across different runs of k-means, at least for this problem.\n",
    "* **K-means can't figure out the number of clusters**. Often we won't know the correct number of clusters and would rather the algorithm figure it out for us.\n",
    "* **K-means does not care about density**. It does not try to draw cluster boundaries where the density is lowest.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5447d13-00b9-49fc-ac02-dbdff1cda52c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Additional Resources\n",
    "\n",
    "* [k-means at Wikipedia](https://en.wikipedia.org/wiki/K-means_clustering)\n",
    "* [k-means user-guide at Scikit-learn](https://scikit-learn.org/stable/modules/clustering.html#k-means)\n",
    "* [k-means documentation at Scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
