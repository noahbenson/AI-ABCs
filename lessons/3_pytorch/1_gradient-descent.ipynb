{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7df3ebe2-138f-4f78-969d-1d28a4db525f",
   "metadata": {},
   "source": [
    "# Optimization and Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36493bd-91f8-45a9-8a12-90c96c74b7fe",
   "metadata": {},
   "source": [
    "Optimization is a broad topic in machine learning; we will focus on a specific (but large and powerful) subset of optimization in which an algorithm attempts to learn the parameters of a function that minimize its (continuous real-valued) output. If one can perform this kind of optimization, one can also use it to learn the parameters of a model that best explains a set of data. PyTorch excels at both of these methods, which are the foundation of the next lesson on neural networks. We'll start by examining the former then use it to construct the latter method in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c5a744-9f0c-4248-a654-725eb54260f0",
   "metadata": {},
   "source": [
    "## How does gradient descent optimization work?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b582805-3813-422f-b4d8-814737daa80f",
   "metadata": {},
   "source": [
    "The basic idea of gradient descent is that the gradient of a function points in the direction it is increasing the fastest and its opposite is the direction in which it decreases the fastest, so if you make a guess at the parameters that minimize the function then calculate the gradient at that point, it tells you the direction of a point that's closer to the minimum. You can then take a small step in that direction and repeat the process until you're as close as you need to be.\n",
    "\n",
    "We'll unpack the description of gradient descent immediately above using a simple example. Suppose we are looking for the minimum of the following function:\n",
    "\n",
    "$$ f(x, y) = (x - 1)^2 + 4(y + 1)^2 + \\frac{x y}{2} $$\n",
    "\n",
    "This function doesn't mean anything to us in particular, but we'll use it as an example. In this example, $f$ is our *loss* function&mdash;i.e., the real-valued continuous function that we are trying to minimize. Here, $x$ and $y$ are the parameters that we are learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b395de6c-4542-405e-b7b7-23294f58dc75",
   "metadata": {},
   "source": [
    "## Example: finding the minimum of a simple function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24f719f-d0aa-4b01-bc32-09ccff7f2ec7",
   "metadata": {},
   "source": [
    "We'll start by implementing the function $f$ (from above) in Python code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea9b644-0100-45d0-802d-698370e23edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# This function will work fine with PyTorch tensors or with NumPy arrays\n",
    "# as long as both x and y are the same type.\n",
    "def f(x, y):\n",
    "    return (x - 1)**2 + 4*(y + 1)**2 + x*y/2\n",
    "\n",
    "# Test that it works:\n",
    "f(3, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66960d25-9760-42c1-b0f3-cdac6ebe48ce",
   "metadata": {},
   "source": [
    "Notice that we can make tensors out of individual numbers (individual numbers are just rank-0 tensors), and when we perform calculations using these tensor numbers, they yield tensor numbers as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a473359-abdb-4810-b905-59d56ed31c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(3.0)\n",
    "y = torch.tensor(3.0)\n",
    "\n",
    "f(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a98c3e-6392-4d03-89e3-41ddd4ba307b",
   "metadata": {},
   "source": [
    "### The `requires_grad` option enables gradient calculation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ebbdde-076a-414e-bd34-d983eaf7da16",
   "metadata": {},
   "source": [
    "Recall that the `torch.tensor` function and similar tensor-creation functions like `torch.ones` and `torch.zeros` accept an optional argument `requires_grad`. This option tells a tensor that it's a parameter to a function involved in some kind of optimization, and so the gradient of this value with respect to some computed value may be required. Let's look at an example of this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d4bc2e-2f1b-4bce-9cba-7a23f6a32b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(3.0, requires_grad=True)\n",
    "y = torch.tensor(3.0, requires_grad=True)\n",
    "\n",
    "# Calculate the value of the function:\n",
    "z = f(x, y)\n",
    "\n",
    "# Ask PyTorch to backward-propagate the gradient of z to its parameters\n",
    "# (this calculates the gradient of z with respect to its parameters x and y\n",
    "# then tells those parameters what their components of the gradients are).\n",
    "z.backward()\n",
    "\n",
    "# Now we can examine the gradient of f(x,y) with respect to x and y:\n",
    "print('∂f/∂x =', x.grad)\n",
    "print('∂f/∂y =', y.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc970bb-4c6f-4f44-816e-c13cf9e0f8e3",
   "metadata": {},
   "source": [
    "The `requires_grad` option can be very powerful and is required for all parameters in PyTorch optimizations. It does induce a few changes to the way that that one interacts with the tensor, however. These changes are required because PyTorch needs to carefully keep track of all the calculations that result from any tensors that require gradients&mdash;if a tensor involved in these calculations gets updated outside of PyTorch's ecosystem, it can lose track of critical parts of the calculation and create incorrect gradients.\n",
    "\n",
    "Primarily, when `requires_grad` is enabled, accessing the NumPy array of a PyTorch tensor requires an extra step, and in-place operations become illegal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940cc5ba-099d-46fe-9ab1-7b72ae2ed26a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "# For normal tensors of single numbers, we can update the tensor by using an\n",
    "# empty tuple in the setitem paradigm (tensor[()] = new_value):\n",
    "tens = torch.tensor(0.0)\n",
    "tens[()] = 1.0\n",
    "print('tens:', tens)\n",
    "\n",
    "# But for a tensor that requires gradient, one cannot update the tensor\n",
    "# directly because this would ruin the gradient tracking.\n",
    "x[()] = 10  # This will raise an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce160f03-20b2-471b-8443-9da689074749",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "# For a normal tensor, we can access it's NumPy array using the x.numpy()\n",
    "# method.\n",
    "tens_np = tens.numpy()\n",
    "print('tens_np:', tens_np)\n",
    "\n",
    "# For a tensor that requires gradient, this will raise an error, because\n",
    "# editing the NumPy tensor would ruin the gradient tracking (and once PyTorch\n",
    "# returns the array, it can't stop you from editing it).\n",
    "print(x.numpy())  # This will raise an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f837c13-2a00-4f95-997b-c4a03bbeba21",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "# The same is true of any tensor involved in gradient tracking; because z is\n",
    "# the result of a computation that included a tensor with requires_grad=True,\n",
    "# z cannot be directly accessed or edited either.\n",
    "print(z.numpy())  # This will raise an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e516e35-d4f7-4024-aeb6-b812dc690d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As the error message suggests, we have to detach a tensor that is part of\n",
    "# any gradient tracking before we extract a numpy array.\n",
    "print(x.detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae54f490-493f-48c5-90fe-84522363c837",
   "metadata": {},
   "source": [
    "The `detach()` method can be used to make a duplicate PyTorch tensor that no longer has `requires_grad` set to `True`.\n",
    "\n",
    "```{warning}\n",
    "Although `x.detach()` returns a new PyTorch tensor that has been detached from the gradient tracking system, it still uses the same memory under the hood to store the tensor elements as the original PyTorch tensor. This means that if you edit the detached tensor or its associated NumPy array, it can produce errors when gradients are computed. If you plan to edit these arrays you should copy them first!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d130c6-4455-45eb-90bb-57c3e639b655",
   "metadata": {},
   "source": [
    "### The Optimization Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a76a3f2-2e3e-47bb-93ed-fc3ed0fdab5e",
   "metadata": {},
   "source": [
    "To perform the minimization, we'll start by making a guess as to $x$ and $y$ values that yield the minimum. (It may not be a very good guess.) We'll create an optimizer (a PyTorch class) that manages the state of the optimization. We'll then take repeated steps toward the minimum using the gradient. (PyTorch mostly does this automatically for us.) In each step, we'll perform a few sub-steps:\n",
    "1. **Calculate the value of the function at $x$ and $y$ (`z = f(x, y)`).** The value `z` that is returned is a PyTorch tensor.\n",
    "2. **Calculate and propogate the gradient backward from `z` to its parameters `x` and `y`.** PyTorch does this step for us via the method `z.backward()`.\n",
    "3. **Tell the optimizer to take a step.** PyTorch does the work in this step, which involves updating all of the parameters (`x` and `y` in this case), to be a little closer to the minimum.\n",
    "\n",
    "```{note}\n",
    "The number of steps to take is a hyperparemter of the optimization. A larger number of steps will usually result in a better optimization, but the improvement in the optimization usually diminishes with each step.\n",
    "\n",
    "Alternatively, one can use a heuristic to decide when to stop, such as choosing to take steps until the change in the function value is smaller than some fixed value.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff664ff-b6b3-4140-b43c-5cbe530b9e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are a few hyperparameters we can declare ahead of time.\n",
    "\n",
    "# The number of steps determins how many minimization steps we take.\n",
    "n_steps = 50\n",
    "\n",
    "# The learning rate is essentially a knob we can turn to try to speed up or\n",
    "# slow down the optimization. A higher learning rate means that the optimizer\n",
    "# takes larger steps along the gradient; a lower learning rate means that it\n",
    "# takes smaller steps. Small steps converge more slowly, but if you take too\n",
    "# large of a step you can pass the minimum and potentially get farther away.\n",
    "# The best learning rate will depend on the optimizer and the function being\n",
    "# optimized, so it often has to be found experimentally.\n",
    "lr = 0.1\n",
    "\n",
    "# Now that we've declared our hyperparameters, let's declare our parameters.\n",
    "x = torch.tensor(3.0, requires_grad=True)\n",
    "y = torch.tensor(3.0, requires_grad=True)\n",
    "\n",
    "# Next, we can declare an optimizer.\n",
    "# We'll use the optimizer SGD: stochastic gradient descent.\n",
    "# The \"stochastic\" refers to the way it handles training when a dataset is\n",
    "# involved and does not indicate that this method is stochastic in this case.\n",
    "# The optimizer will manage the steps and updating the parameters during them.\n",
    "# Because of this we have to tell it the parameters and the learning rate.\n",
    "optimizer = torch.optim.SGD([x, y], lr=lr)\n",
    "\n",
    "# Now we can take several optimization steps:\n",
    "for step_number in range(n_steps):\n",
    "    # We're starting a new step, so we reset the gradients.\n",
    "    optimizer.zero_grad()\n",
    "    # Calculate the function value at these parameters.\n",
    "    z = f(x, y)\n",
    "    # Have PyTorch backward-propagate the gradients.\n",
    "    z.backward()\n",
    "    # If the norm of the gradient is less than 1e-5, we finish early.\n",
    "    if torch.hypot(x.grad, y.grad) < 1e-4:\n",
    "        break\n",
    "    # Print a message about this step:\n",
    "    print(\"Step number\", step_number)\n",
    "    print(\"  x = \", float(x), \";  ∂f/∂x = \", float(x.grad))\n",
    "    print(\"  y = \", float(y), \";  ∂f/∂y = \", float(y.grad))\n",
    "    print(\"  z = \", float(z))\n",
    "    # Have the optimizer take a step:\n",
    "    optimizer.step()\n",
    "\n",
    "# After the optimizer has run, print out what it's found:\n",
    "print(\"Final result:\")\n",
    "print(f\"  f({float(x)}, {float(y)}) = {float(z)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b17b2f-f06e-4c35-99bc-d1e9095010d7",
   "metadata": {},
   "source": [
    "### Effects of the learning rate parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58aaa99e-53bb-4dcc-81e0-bb9f965b7817",
   "metadata": {},
   "source": [
    "We can try running the above code-block with different hyperparameters to get a sense of their impact. Try running the above block using a high learning rate (`lr=0.5`) and a low learning rate (`lr=0.01`).\n",
    "\n",
    "With `lr=0.01`, the optimization works fine, but it doesn't finish. It gets closer to the minimum, but it doesn't reach the minimum value found with `lr=0.1`.\n",
    "\n",
    "With `lr=0.5` the optimization actually diverges! This means that it takes steps so large that they go so past the minimum to a point that's higher than the start point.\n",
    "\n",
    "To get a better sense for what these mean, let's make some plots of the steps that the optimizer takes. Try running this with different `lr` parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e56d19-241a-45e4-a911-6e4cd14aed42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Declare our hyperparameters:\n",
    "n_steps = 50\n",
    "lr = 0.1\n",
    "\n",
    "# Now the parameters:\n",
    "x = torch.tensor(3.0, requires_grad=True)\n",
    "y = torch.tensor(3.0, requires_grad=True)\n",
    "\n",
    "# Now the optimizer:\n",
    "optimizer = torch.optim.SGD([x, y], lr=lr)\n",
    "\n",
    "# Now we set up a pyplot figure.\n",
    "(fig,ax) = plt.subplots(1, 1, figsize=(3,3), dpi=288)\n",
    "fig.subplots_adjust(0, 0, 1, 1, 0, 0)\n",
    "# Make an image of the function itself and plot it as the background.\n",
    "(x_im, y_im) = np.meshgrid(\n",
    "    np.linspace(-5,5,512),\n",
    "    np.linspace(-5,5,512),\n",
    "    indexing='xy')\n",
    "z_im = f(x_im, y_im)\n",
    "ax.imshow(z_im, cmap='jet', zorder=-1, extent=(-5,5,5,-5))\n",
    "ax.invert_yaxis()\n",
    "\n",
    "# Now we can take several optimization steps:\n",
    "for step_number in range(n_steps):\n",
    "    # We're starting a new step, so we reset the gradients.\n",
    "    optimizer.zero_grad()\n",
    "    # Calculate the function value at these parameters.\n",
    "    z = f(x, y)\n",
    "    # Have PyTorch backward-propagate the gradients.\n",
    "    z.backward()\n",
    "    # Plot the points and the gradients:\n",
    "    (x_np, y_np) = (x.detach().numpy(), y.detach().numpy())\n",
    "    (x_np, y_np) = (np.array(x_np), np.array(y_np))\n",
    "    ax.plot(x_np, y_np, 'w.', ms=0.5)\n",
    "    # If the norm of the gradient is less than 1e-5, we finish early.\n",
    "    if torch.hypot(x.grad, y.grad) < 1e-4:\n",
    "        break\n",
    "    # Have the optimizer take a step:\n",
    "    optimizer.step()\n",
    "    # Plot the arrow from the previous to this point.\n",
    "    dx_np = x.detach().numpy() - x_np\n",
    "    dy_np = y.detach().numpy() - y_np\n",
    "    ax.arrow(x_np, y_np, dx_np, dy_np, color='w', lw=0.25, head_width=0.06)\n",
    "\n",
    "# After the optimizer has run, show the steps:\n",
    "ax.set_xlim([-5,5])\n",
    "ax.set_ylim([-5,5])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0494cdf-3209-4ade-a959-d7c89c3eea78",
   "metadata": {},
   "source": [
    "## Another Example: The problem of local minima."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505ee387-cd4e-42af-8c97-2d221280a10d",
   "metadata": {},
   "source": [
    "Let's examine a different function. We'll use essentially the same optimization loop but will define a slightly different function:\n",
    "\n",
    "$$ g(x, y) = \\left((x + 1)^2 + (y + 1)^2\\right) \\left((x - 3)^2 + (y - 3)^2 + 1\\right) $$\n",
    "\n",
    "Let's defint this function then run our optimization again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e35932-18ce-40c7-bb27-83428f3de637",
   "metadata": {},
   "outputs": [],
   "source": [
    "def g(x, y):\n",
    "    return ((x + 1)**2 + (y + 1)**2) * ((x - 3)**2 + (y - 3)**2 + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47a6aaa-0b90-41f2-950c-ce8d06572400",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare our hyperparameters:\n",
    "n_steps = 50\n",
    "lr = 0.005  # This function is different and needs a lower learning rate.\n",
    "\n",
    "# Now the parameters; we'll start at slightly different position this time.\n",
    "x = torch.tensor(3.5, requires_grad=True)\n",
    "y = torch.tensor(4.0, requires_grad=True)\n",
    "\n",
    "# Now the optimizer:\n",
    "optimizer = torch.optim.SGD([x, y], lr=lr)\n",
    "\n",
    "# Now we set up a pyplot figure.\n",
    "(fig,ax) = plt.subplots(1, 1, figsize=(3,3), dpi=288)\n",
    "fig.subplots_adjust(0, 0, 1, 1, 0, 0)\n",
    "# Make an image of the function itself and plot it as the background.\n",
    "(x_im, y_im) = np.meshgrid(\n",
    "    np.linspace(-5,5,512),\n",
    "    np.linspace(-5,5,512),\n",
    "    indexing='xy')\n",
    "z_im = g(x_im, y_im)\n",
    "# We can add a vmax to make sure our visualization captures the part of the\n",
    "# image that is of interest to us.\n",
    "ax.imshow(z_im, cmap='jet', zorder=-1, extent=(-5,5,5,-5), vmax=500)\n",
    "ax.invert_yaxis()\n",
    "\n",
    "# Now we can take several optimization steps:\n",
    "for step_number in range(n_steps):\n",
    "    # We're starting a new step, so we reset the gradients.\n",
    "    optimizer.zero_grad()\n",
    "    # Calculate the function value at these parameters.\n",
    "    z = g(x, y)\n",
    "    # Have PyTorch backward-propagate the gradients.\n",
    "    z.backward()\n",
    "    # Plot the points and the gradients:\n",
    "    (x_np, y_np) = (x.detach().numpy(), y.detach().numpy())\n",
    "    (x_np, y_np) = (np.array(x_np), np.array(y_np))\n",
    "    ax.plot(x_np, y_np, 'w.', ms=0.5)\n",
    "    # If the norm of the gradient is less than 1e-5, we finish early.\n",
    "    if torch.hypot(x.grad, y.grad) < 1e-4:\n",
    "        break\n",
    "    # Have the optimizer take a step:\n",
    "    optimizer.step()\n",
    "    # Plot the arrow from the previous to this point.\n",
    "    dx_np = x.detach().numpy() - x_np\n",
    "    dy_np = y.detach().numpy() - y_np\n",
    "    ax.arrow(x_np, y_np, dx_np, dy_np, color='w', lw=0.25, head_width=0.06)\n",
    "\n",
    "# After the optimizer has run, show the steps:\n",
    "ax.set_xlim([-5,5])\n",
    "ax.set_ylim([-5,5])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9772994e-4836-459e-8095-ceaef2222494",
   "metadata": {},
   "source": [
    "Clearly in the above example, the method found a minimum value, but there's another minimum value in this function, and it's lower than the minimum that the optimization method found. What if we change the initial parameters `x` and `y` to have a start value closer to the other minimum?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0992d574-7b86-454f-8bef-9fcdc785d88d",
   "metadata": {},
   "source": [
    "Local minima are an issue in many optimization problems. In some cases, it can be proven that a global minimum has been found, but in many cases it cannot be. One strategy for avoiding local minima is to fit a function many times and to use, as the final result, whatever lowest value was achieved across all runs. Of course, this is quite time consuming, and even with many random starts, there's no guarantee that the global minimum will be found. While a broader discussion of local minima is beyond the scope of this course, it is always important to think about the possibility of local minima in any optimization one is performing."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
