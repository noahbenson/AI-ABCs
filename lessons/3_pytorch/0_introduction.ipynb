{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8936d36e-6256-491c-95f9-9dcdf85997ac",
   "metadata": {},
   "source": [
    "# Introduction to PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63fa43bb-73b8-4104-a424-2c0377ea666e",
   "metadata": {},
   "source": [
    "PyTorch is one of the preeminent machine-learning and optimization libraries currently available. It contains a number of powerful features that drastically simplify the task of fitting models and training neural networks. While we won't have time in this tutorial to examine more than a few of the core features, there are many additional tutorials available online. This tutorial will be roughly similar to a few of the introductory PyTorch tutorials available at [pytorch.org/tutorials](https://pytorch.org/tutorials).\n",
    "\n",
    "PyTorch has many features and utilities, but at its core there are just a few pieces that contribute most to its impact on the AI/ML community:\n",
    "* **Tensor Operations**. PyTorch provides most of the same numerical processing utilities as the NumPy library (and, in fact, the core interface of PyTorch is very similar to NumPy's, as we will see later in this lesson).\n",
    "* **GPU Support**. PyTorch allows one to write Python code that can be easily run on either a GPU, if available, or on the CPU.\n",
    "* **Automatic Differentiation**. A substantial portion of the field of numerical optimization is based on the use of the derivatives and gradients of a function to find its minimum, i.e. \"gradient descent\" optimization. Efficiently calculating the gradient of an arbitrary function that is associated with a model or a machine learning model (such as a neural network) can be very difficult, however. PyTorch performs this calculation automatically for you, making many kinds of optimizations much easier.\n",
    "* **Tools for Model Training**. Finally, PyTorch includes a variety of optimization tools and data management utilities for use in training models from data. These tools include classes that implement the computations of neural networks and convolutional neural networks.\n",
    "\n",
    "In this lesson, we will take a look at each of these features and work through some examples of nonlinear models using the California Housing Dataset. In the next lesson, [](/4_neural-networks/0_introduction), we will look at neural networks and convolutional neural networks in PyTorch, specifically."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c258fe5a-60bc-4ea7-910a-857a94a5fd19",
   "metadata": {},
   "source": [
    "## Installing PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5d366f-3978-4bbb-b6e2-3717050bb414",
   "metadata": {},
   "source": [
    "If you're using the Docker setup provided with the [AI ABCs GitHub repository](https://github.com/noahbenson/AI-ABCs/) then PyTorch should already be installed for you; however, you will not be able to execute PyTorch code on a GPU using the Docker image. (You do not need to be able to use a GPU to do anything in this course.)\n",
    "\n",
    "To install PyTorch locally, it is strongly recommended that you use an environment manager like [conda](https://docs.conda.io/en/latest/). PyTorch can be installed using `conda` via the command `conda install -c pytorch pytorch`. It can be installed using `pip` via the command `pip install torch`. The library itself is called `torch`.\n",
    "\n",
    "Additionally, the website [pytorch.org](https://pytorch.org/) maintains a [get-started page](https://pytorch.org/get-started/locally/) that contains installation instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5fc0ab-8323-4750-b945-79319a25ffcf",
   "metadata": {},
   "source": [
    "## Basic Operations in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cab2029-a181-47b1-8444-cee4bf6e061b",
   "metadata": {},
   "source": [
    "To start using PyTorch, we will first need to import it. The library is called `torch` in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1e4e63-c8d8-4e4f-9528-cc3178b685b6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# We'll also want numpy to compare to:\n",
    "import numpy as np\n",
    "\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8bb2ed-cd96-4df8-9e27-9b6063ed6d63",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### PyTorch's `Tensor` is a lot like NumPy's `ndarray`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a08f29-b15b-4970-90e0-f94a4be9099a",
   "metadata": {},
   "source": [
    "At first glance, PyTorch appears to be somewhat like NumPy in that it gives the user a set of classes and functions for interacting with a `Tensor` type that behaves much like NumPy's `ndarray` type. Both NumPy and PyTorch, for example, define functions like `log`, `sin`, and `mean` that work with their respective array type. However, the `Tensor` and `ndarray` objects aren't interchangeable because PyTorch `Tensor`s are intended for use in optimization problems and thus potentially keep track of extra data. These data are critical for performing efficient gradient-descent parameter-tuning, which is generally required for optimization such as training neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0af8a0-9453-4f22-9dfd-dce70cbc93d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a PyTorch Tensor object (like a numpy.ndarray object):\n",
    "tens = torch.tensor([1.0, 2.5, 4.0])\n",
    "\n",
    "print(type(tens))\n",
    "\n",
    "tens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1df579-d5bc-43c1-b7eb-906e63cd4d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensors support basic arithmatic and comparison operations, like arrays:\n",
    "print('+', tens + tens)\n",
    "print('/', tens / tens)\n",
    "print('==', tens == tens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66698d59-a411-4cfe-9b04-ca466615d9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch includes many of the same basic numerical functions as NumPy too:\n",
    "print('exp', torch.exp(tens))\n",
    "print('mean', torch.mean(tens))\n",
    "print('sum', torch.sum(tens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d702b9f-9ff2-4410-9a7a-d42e02dda992",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensors can have many dimensions, just like NumPy arrays:\n",
    "mtx = torch.reshape(torch.linspace(0, 1, 6), (2, 3))\n",
    "mtx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec98543d-71a4-476a-9f2c-a28699e5774f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accumulation functions like sum and mean typically take axis options, just\n",
    "# like in NumPy. PyTorch likes to call this 'dim' instead of 'axis', but it\n",
    "# typically accepts either version:\n",
    "print('axis=0', torch.sum(mtx, axis=0))\n",
    "print('dim=1', torch.sum(mtx, dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b54515-18b1-4eb7-a4ec-98ac72a85932",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Tensors have shapes and dtypes, but unlike NumPy arrays, Tensors only\n",
    "# support a specific set of PyTorch numeric dtypes and are accessed in ways\n",
    "# similar to NumPy arrays.\n",
    "print('shape:', mtx.shape)\n",
    "print('dtype:', mtx.dtype)\n",
    "print('column:', mtx[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebd5d10-6461-4795-a1dd-efbb24c29ace",
   "metadata": {},
   "source": [
    "### Differences between `Tensor` and `ndarray`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab160803-5fb0-422c-81ee-5581f17fd585",
   "metadata": {},
   "source": [
    "Despite the broad similarities between `Tensor` and `ndarray`, there are a number of differences as well. First of all, `Tensor`s can only store numerical data&mdash;they cannot store strings or Python objects the way NumPy arrays can. Additionally, the `Tensor` type has a `device` parameter that can be provided to the `torch.tensor` function and a number of similar functions that return new PyTorch tensors. The `device` parameter is necessary because it is often necessary to have numerical data in certain memory buffers in order for them to be processed by a peripheral processing unit like a GPU. In such a case, the device is typically set to something like `device='cuda'`, but this will depend on the system. The value `device='cpu'` can be used to allocate tensors explicitly for the CPU. Finally, `Tensor`s have a parameter `requiers_grad` that is used in gradient computations; we will discuss both of these options in upcoming sections of this lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db57726-cfcf-4dbd-b548-bbc3f9dbaa01",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "# PyTorch tensors can't be made of strings:\n",
    "x = torch.tensor(['a', 'b', 'c'])  # This will error!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd63c4e-b07f-4a91-b622-541cc4166b3c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# By default the device of a tensor will be the cpu and requires_grad will be\n",
    "# False.\n",
    "print('device:', tens.device)\n",
    "print('requires_grad:', tens.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159a3bb8-0868-4f34-a654-ad2392097885",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### PyTorch / NumPy Interface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033a5ce4-1d86-4ad9-bbb5-f67d002039d0",
   "metadata": {},
   "source": [
    "Although PyTorch has a very similar interface to NumPy and even uses NumPy arrays under the hood, most of its functions aren't compatible with NumPy arrays. In fact, most PyTorch functions don't work unless their arguments are tensors. If you are used to NumPy's broadly permissive approach to its arguments&mdash;for example, `np.sum([1,2])` is perfectly valid, even though the argument is a list of numbers rather than a NumPy array&mdash;then PyTorch's approach may feel initially very strict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1b0eec-6315-4a39-b93d-fcfb7e45c55e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "# This will raise an exception because the argument is not a Tensor.\n",
    "torch.sum([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5e0049-f2a3-4d46-8df9-3e156e0f282e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "# Similarly, NumPy arrays aren't PyTorch tensors:\n",
    "torch.sum(np.array([1,2,3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1e25b4-ad94-4926-bc31-3d973456bff6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "# Tensors won't sum with lists (arrays will), so this will also error:\n",
    "tens + [1,2,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1ecd7e-8330-49c1-a5ed-0dbf98a7ae1a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Because Tensors are iterables, NumPy arrays will add with Tensors (returning\n",
    "# new array objects), but the preferred way of doing this either cast the\n",
    "# array to a tensor or vice versa so that it is clear which type is the result\n",
    "# should be).\n",
    "\n",
    "# Example NumPy array:\n",
    "arr = np.array([1.0, 2.0, 3.0])\n",
    "\n",
    "print('Cast array to tensor:', tens + torch.from_numpy(arr))\n",
    "print('Cast tensor to array:', tens.numpy() + arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b2a5ff-4eea-4332-8e1a-e3882b424951",
   "metadata": {},
   "source": [
    "## PyTorch and GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c1b937-89ee-4872-a315-404e40055fcf",
   "metadata": {},
   "source": [
    "One great part of PyTorch is that it can flexibly be used with either the CPU or peripheral processors like GPUs. (Configuring PyTorch to use GPUs is beyond the scope of this course; we recommend [PyTorch's getting started page](https://pytorch.org/get-started/locally/) for help with GPU configuration.) If an operation or model runs on the CPU in PyTorch, you can be fairly confident that it will run on a GPU once PyTorch has been configured to use that GPU. GPUs can substantially speed up the training of many ML algorithms, especially the neural networks and convolutional neural networks that we will see in the [next lesson](/4_neural-networks/0_introduction).\n",
    "\n",
    "PyTorch's Tensors include a method `to` that can be used to move a tensor from one device (like the CPU) to another (like a GPU). Typically this would look something like the following:\n",
    "\n",
    "```python\n",
    "tens_cuda = tens.to('cuda')\n",
    "# Or:\n",
    "tens_cuda = tens.cuda()\n",
    "```\n",
    "\n",
    "If you're on a system with multiple GPUs, then `'cuda:0'` refers to the first GPU, `'cuda:1'` refers to the second GPU, etc., with `'cuda'` by itself referring to the current selected GPU, which can be set using the function `torch.cuda.set_device` (e.g., `torch.cuda.set_device(0)` for `'cuda:0'`). To find out how many GPUs are available on a given system, you can run `torch.cuda.device_count()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de837106-ebf1-41bf-a86b-7077f6e49ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will return 0 if there are no GPUs available.\n",
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0286c6-81d2-440d-b351-2c5031772fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function torch.cuda.is_available is basically a synonym for\n",
    "# (torch.cuda.device_count() > 0):\n",
    "torch.cuda.is_available()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
